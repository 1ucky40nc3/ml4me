{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhbILbYZgxOdI41uiEM6El",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1ucky40nc3/ml4me/blob/main/vision/HQ-SAM/run_hq_sam_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2023 Louis Wendler\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "ZgY9-NAcZ22g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A [**Segment Anything in High Quality (HQ-SAM)**](https://github.com/syscv/sam-hq) Demo\n"
      ],
      "metadata": {
        "id": "NE1RkP1PZF4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Notebook"
      ],
      "metadata": {
        "id": "BnTnL2Y7Ydz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GUkCfmD948E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWiXJOXv4Nn6"
      },
      "outputs": [],
      "source": [
        "# @title Clone the [`HQ-SAM`](https://github.com/syscv/sam-hq) Repository\n",
        "!git clone https://github.com/SysCV/sam-hq.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download a Pretrained SAM Model\n",
        "import os\n",
        "\n",
        "\n",
        "checkpoint_dir = \"/content/ckpts\"\n",
        "checkpoint_url = \"https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth\"\n",
        "checkpoint_path = os.path.join(\n",
        "    checkpoint_dir,\n",
        "    os.path.split(checkpoint_url)[-1]\n",
        ")\n",
        "\n",
        "!mkdir -p $checkpoint_dir\n",
        "!wget $checkpoint_url -P $checkpoint_dir"
      ],
      "metadata": {
        "id": "C74_kyBS4YZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Implement Utils\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.axes import Axes\n",
        "\n",
        "\n",
        "def show_mask(mask: np.ndarray, ax: Axes) -> None:\n",
        "    color = np.concatenate([\n",
        "            np.random.random(3),\n",
        "            np.array([0.6])\n",
        "        ],\n",
        "        axis=0\n",
        "    )\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask)\n",
        "\n",
        "\n",
        "def show_points(\n",
        "    coords: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    ax: Axes,\n",
        "    marker_size: int=375\n",
        ") -> None:\n",
        "    for i, color in ['red', 'green']:\n",
        "        points = coords[labels==i]\n",
        "        ax.scatter(\n",
        "            points[:, 0],\n",
        "            points[:, 1],\n",
        "            color=color,\n",
        "            marker='*',\n",
        "            s=marker_size,\n",
        "            edgecolor='white',\n",
        "            linewidth=1.25\n",
        "        )\n",
        "\n",
        "\n",
        "def show_results(\n",
        "    masks: np.ndarray,\n",
        "    scores: np.ndarray,\n",
        "    point_coords: np.ndarray,\n",
        "    point_labels: np.ndarray,\n",
        "    image: np.ndarray\n",
        ") -> None:\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        print(f\"Score: {score:.3f}\")\n",
        "\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(image)\n",
        "        show_mask(mask, plt.gca())\n",
        "        show_points(point_coords, point_labels, plt.gca())\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ph2Zfe9949hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load a Pretrained SAM Model\n",
        "%cd /content/sam-hq\n",
        "\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# @markdown Select the model type\n",
        "model_type = \"vit_l\" # @param ['default', 'vit_h', 'vit_l', 'vit_b']\n",
        "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "sam.to(device=device)\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "GIAxwC2V5-_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do some Segementation!"
      ],
      "metadata": {
        "id": "pkiB7PrlYier"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare the SAM Prompts\n",
        "import IPython\n",
        "from IPython.display import HTML\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "def load_data_url(path: str) -> str:\n",
        "    mediatype_map = {\n",
        "        \".png\": \"image/png\",\n",
        "        \".jpg\": \"image/jpeg\"\n",
        "    }\n",
        "    ext = os.path.splitext(path)[-1]\n",
        "    mediatype = mediatype_map[ext]\n",
        "    data = open(path, 'rb').read()\n",
        "    data = b64encode(data).decode()\n",
        "    return f\"data:{mediatype};base64,{data}\"\n",
        "\n",
        "\n",
        "coordinates = []\n",
        "labels = []\n",
        "\n",
        "def click_coordinates_callback(x: int, y: int, left_click: bool) -> None:\n",
        "    global coordinates\n",
        "    coordinates.append((x, y))\n",
        "    global labels\n",
        "    label = int(left_click)\n",
        "    labels.append(label)\n",
        "\n",
        "\n",
        "output.register_callback(\n",
        "    \"notebook.ClickCoordinates\",\n",
        "    click_coordinates_callback\n",
        ")\n",
        "\n",
        "# @markdown Select an input image\n",
        "image_path = \"/content/sam-hq/demo/input_imgs/example0.png\" # @param {type: \"string\"}\n",
        "# @markdown Set the image width\n",
        "width = 100 # @param {type: \"number\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown Select image pixels as prompts for the SAM model:\n",
        "\n",
        "# @markdown `-->` Click on the image below:\n",
        "# @markdown\n",
        "# @markdown *   `Left` click: Positive prompt (segmentation target)\n",
        "# @markdown *   `Right` click: Negative prompt (avoid segmentation)\n",
        "\n",
        "# @markdown **Hint**: We show your prompts after processing the prompt-image-pair in the cell below.\n",
        "\n",
        "display(IPython.display.HTML('''\n",
        "<img src=\"%s\" width={width}/>\n",
        "<script>\n",
        "    document.querySelector(\"img\").addEventListener(\"click\", function(event) {\n",
        "        var x = event.pageX - this.offsetLeft;\n",
        "        var y = event.pageY - this.offsetTop;\n",
        "\n",
        "        var isRightClick = false;\n",
        "        if (\"which\" in event)  // Gecko (Firefox), WebKit (Safari/Chrome) & Opera\n",
        "            isRightClick = event.which == 3;\n",
        "        else if (\"button\" in event)  // IE, Opera\n",
        "            isRightClick = event.button == 2;\n",
        "\n",
        "        google.colab.kernel.invokeFunction(\n",
        "            'notebook.ClickCoordinates',\n",
        "            [x, y, !isRightClick],\n",
        "            {}\n",
        "        );\n",
        "    });\n",
        "    document.querySelector(\"img\").addEventListener(\"contextmenu\", function(event) {\n",
        "        var x = event.pageX - this.offsetLeft;\n",
        "        var y = event.pageY - this.offsetTop;\n",
        "\n",
        "        google.colab.kernel.invokeFunction(\n",
        "            'notebook.ClickCoordinates',\n",
        "            [x, y, false],\n",
        "            {}\n",
        "        );\n",
        "    });\n",
        "</script>\n",
        "''' % load_data_url(image_path)))"
      ],
      "metadata": {
        "id": "0HrXYyjeNcad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the SAM Inference\n",
        "def load_img(path: str) -> np.ndarray:\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "\n",
        "image = load_img(image_path)\n",
        "predictor.set_image(image)\n",
        "\n",
        "point_coords = np.array(coordinates)\n",
        "point_labels = np.array(labels)\n",
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=point_coords,\n",
        "    point_labels=point_labels,\n",
        "    multimask_output=False,\n",
        "    hq_token_only=False,\n",
        ")\n",
        "show_results(masks, scores, point_coords, point_labels, image)"
      ],
      "metadata": {
        "id": "lTah41drO4qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgments\n",
        "\n",
        "---\n",
        "\n",
        "Thanks to the original [HQ-SAM](https://github.com/syscv/sam-hq) and [SAM](https://github.com/facebookresearch/segment-anything) authors!"
      ],
      "metadata": {
        "id": "YKtKvFB9Sax2"
      }
    }
  ]
}