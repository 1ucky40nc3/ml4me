{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNvY8TovA_nc"
      },
      "source": [
        "# E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-gI3tb0Bss1"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-QFQ8Z2B0QW"
      },
      "source": [
        "- üìÉ [Paper](https://wty-ustc.github.io/inversion/paper/E2Style.pdf)\n",
        "- üìö [Project Page](https://wty-ustc.github.io/inversion)\n",
        "- üé¨ [Examples](https://youtu.be/gJwFgdRHK0M)\n",
        "- üíª [Code](https://github.com/wty-ustc/e2style)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxIgS8WdB4pQ"
      },
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XquN7cwJB71R"
      },
      "source": [
        "[Abstract](https://wty-ustc.github.io/inversion/paper/E2Style.pdf)‚Äî*This paper studies the problem of StyleGAN inversion, which plays an essential role in enabling the pretrained\n",
        "StyleGAN to be used for real image editing tasks. The goal of\n",
        "StyleGAN inversion is to find the exact latent code of the given\n",
        "image in the latent space of StyleGAN. This problem has a high\n",
        "demand for quality and efficiency. Existing optimization-based\n",
        "methods can produce high-quality results, but the optimization\n",
        "often takes a long time. On the contrary, forward-based methods\n",
        "are usually faster but the quality of their results is inferior. In\n",
        "this paper, we present a new feed-forward network ‚ÄúE2Style‚Äù\n",
        "for StyleGAN inversion, with significant improvement in terms\n",
        "of efficiency and effectiveness. In our inversion network, we\n",
        "introduce: 1) a shallower backbone with multiple efficient heads\n",
        "across scales; 2) multi-layer identity loss and multi-layer face\n",
        "parsing loss to the loss function; and 3) multi-stage refinement.\n",
        "Combining these designs together forms an effective and efficient method that exploits all benefits of optimization-based\n",
        "and forward-based methods. Quantitative and qualitative results\n",
        "show that our E2Style performs better than existing forwardbased methods and comparably to state-of-the-art optimizationbased methods while maintaining the high efficiency as well\n",
        "as forward-based methods. Moreover, a number of real image\n",
        "editing applications demonstrate the efficacy of our E2Style. Our\n",
        "code is available at* https://github.com/wty-ustc/e2style\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0xuVVJzCTa-"
      },
      "source": [
        "## Authors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlzn77QiCVm2"
      },
      "source": [
        "Tianyi Wei<sup>1</sup>,\n",
        "Dongdong Chen<sup>2</sup>,\n",
        "Wenbo Zhou<sup>1</sup>,\n",
        "Jing Liao<sup>3</sup>,\n",
        "Weiming Zhang<sup>1</sup>, \n",
        "Lu Yuan<sup>2</sup>, \n",
        "Gang Hua<sup>4</sup>, \n",
        "Nenghai Yu<sup>1</sup> <br>\n",
        "<sup>1</sup>*University of Science and Technology of China,*<br>\n",
        "<sup>2</sup>*Microsoft Cloud AI*<br>\n",
        "<sup>3</sup>*City University of Hong Kong,*<br>\n",
        "<sup>4</sup>*Wormpex AI Research*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsj5eYU9GOuj"
      },
      "source": [
        "## Citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm5EA3BNGbV6"
      },
      "source": [
        "### Plain Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiqG3m7jGd5t"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "T. Wei et al., \"E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion,\" in IEEE Transactions on Image Processing, vol. 31, pp. 3267-3280, 2022, doi: 10.1109/TIP.2022.3167305.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Q9b9jdGQbo"
      },
      "source": [
        "### BibTex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTmEj7wvGVAV"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "@ARTICLE{9760266,\n",
        "  author={Wei, Tianyi and Chen, Dongdong and Zhou, Wenbo and Liao, Jing and Zhang, Weiming and Yuan, Lu and Hua, Gang and Yu, Nenghai},\n",
        "  journal={IEEE Transactions on Image Processing}, \n",
        "  title={E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion}, \n",
        "  year={2022},\n",
        "  volume={31},\n",
        "  number={},\n",
        "  pages={3267-3280},\n",
        "  doi={10.1109/TIP.2022.3167305}}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJC5oUXL9Eif"
      },
      "source": [
        "# Set up the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2G0FFtEu-L0B"
      },
      "outputs": [],
      "source": [
        "# @title ## Mount your Google Drive at `/content/gdrive`\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML9GLyFb9JyK"
      },
      "source": [
        "## Clone the [`e2style`](https://github.com/wty-ustc/e2style) repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbjAiV_8Ogml"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/wty-ustc/e2style.git\n",
        "%cd e2style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOzxE6Yd3WSf"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQrFUJUr3Zn-"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zix_X3zsWzzf"
      },
      "source": [
        "## Download the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rPTWyxJuVZzi"
      },
      "outputs": [],
      "source": [
        "# @markdown Download the following models: [`stylegan2-ffhq-config-f.pt`](https://drive.google.com/file/d/1pts5tkfAcWrg4TpLDu6ILF5wHID32Nzm/view?usp=sharing )\n",
        "# @markdown & [`model_ir_se50.pth`](https://drive.google.com/file/d/1FS2V756j-4kWduGxfir55cMni5mZvBTv/view)\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1pts5tkfAcWrg4TpLDu6ILF5wHID32Nzm/view?usp=sharing \n",
        "!gdown --fuzzy https://drive.google.com/file/d/1FS2V756j-4kWduGxfir55cMni5mZvBTv/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oT-Ltt6TWet7"
      },
      "outputs": [],
      "source": [
        "# @markdown Move the models to the `pretrained_models` directory\n",
        "!cp stylegan2-ffhq-config-f.pt pretrained_models/\n",
        "!cp model_ir_se50.pth pretrained_models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkxszaxaVyeU"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV-b7CB-ZeaY"
      },
      "source": [
        "## Prepare the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN_cxyQqZTai"
      },
      "source": [
        "### Set up the FFQH dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqgKU87G23_B"
      },
      "outputs": [],
      "source": [
        "# @markdown Prepare a directory for the dataset\n",
        "ffhq_dir = \"/content/e2style/data/ffhq\"\n",
        "!mkdir -p $ffhq_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N1yJesUoZ7Bs"
      },
      "outputs": [],
      "source": [
        "# @markdown Copy the dataset from your Google Drive\n",
        "!cp -r /content/gdrive/MyDrive/datasets/ffhq/images1024x1024 $ffhq_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pruIog4GZZKx"
      },
      "source": [
        "### Set up the CelebA-HQ dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfrjLYzYl_5B"
      },
      "outputs": [],
      "source": [
        "# @markdown Prepare a directory for the dataset\n",
        "celeba_dir = \"/content/e2style/data/celeba\"\n",
        "!mkdir -p $celeba_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3950YZukHgP"
      },
      "outputs": [],
      "source": [
        "# @title #### Download the [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset\n",
        "\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1badu11NqxGf6qM3PTTooQDJvQbejgbTv/view\n",
        "!unzip CelebAMask-HQ.zip -d $celeba_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yHStBF71Ymc"
      },
      "source": [
        "#### Set up the CelebA-HQ test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlrYovOZlRYF"
      },
      "outputs": [],
      "source": [
        "# @title ##### Download CelebA dataset's official split configuration\n",
        "!gdown --fuzzy https://drive.google.com/file/d/0B7EVK8r0v71pY0NSMzRuSXJEVkk/view?resourcekey=0-i4TGCi_51OtQ5K9FSp4EDg -O $celeba_dir/list_eval_partition.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSr2a6681w6e"
      },
      "source": [
        "##### Map the local CelebA-HQ dataset according to the split configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0p5c88wmTRN"
      },
      "outputs": [],
      "source": [
        "with open(f\"{celeba_dir}/list_eval_partition.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "list_eval_partition = [l.split(\" \") for l in text.split(\"\\n\")]\n",
        "\n",
        "celeba_test_files = []\n",
        "for l in list_eval_partition:\n",
        "    if len(l) < 2:\n",
        "        continue\n",
        "    f, s = l\n",
        "\n",
        "    if int(s) == 2:\n",
        "        celeba_test_files.append(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvVa3SwWq6mU"
      },
      "outputs": [],
      "source": [
        "with open(f\"{celeba_dir}/CelebAMask-HQ/CelebA-HQ-to-CelebA-mapping.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "lines = text.split(\"\\n\")\n",
        "celeba_mapping = [l.split() for l in lines][1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIJsdK1wpf1p"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "celebahq_files = glob.glob(f\"{celeba_dir}/CelebAMask-HQ/CelebA-HQ-img/*.jpg\")\n",
        "\n",
        "celebahq_test_files = []\n",
        "for f in celebahq_files:\n",
        "    name = f.split(\"/\")[-1]\n",
        "    idx = int(name.split(\".\")[0])\n",
        "    mapped = celeba_mapping[idx][-1]\n",
        "    if mapped in celeba_test_files:\n",
        "        celebahq_test_files.append(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ5_qn8XsMQ7"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "celeba_test_dir = f\"{celeba_dir}/test\"\n",
        "!mkdir -p $celeba_test_dir\n",
        "\n",
        "for f in tqdm(celebahq_test_files):\n",
        "    !cp $celeba_dir/CelebAMask-HQ/CelebA-HQ-img/$f $celeba_test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6UZodVx2uEA"
      },
      "source": [
        "## Setup the dataset paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUurpOdK2xe2"
      },
      "outputs": [],
      "source": [
        "%env CELEBA_DIR=$celeba_dir\n",
        "%env FFHQ_DIR=$ffhq_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKma1wZDYYtH"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/e2style/configs/paths_config.py\n",
        "\n",
        "import os\n",
        "\n",
        "CELEBA_DIR = os.environ[\"CELEBA_DIR\"]\n",
        "FFHQ_DIR = os.environ[\"FFHQ_DIR\"]\n",
        "\n",
        "dataset_paths = {\n",
        "\t'celeba_train': '',\n",
        "\t'celeba_test': f'{CELEBA_DIR}/test',\n",
        "\t'celeba_train_4seg': '',\n",
        "\t'celeba_test_4seg': '',\t\n",
        "\t'celeba_train_sketch': '',\n",
        "\t'celeba_test_sketch': '',\n",
        "\t'celeba_train_segmentation': '',\n",
        "\t'celeba_test_segmentation': '',\n",
        "\t'ffhq': f'{FFHQ_DIR}/images1024x1024',\n",
        "}\n",
        "\n",
        "model_paths = {\n",
        "\t'stylegan_ffhq': 'pretrained_models/stylegan2-ffhq-config-f.pt',\n",
        "\t'ir_se50': 'pretrained_models/model_ir_se50.pth',\n",
        "\t'parsing_net': 'pretrained_models/parsing.pth',\n",
        "\t'circular_face': 'pretrained_models/CurricularFace_Backbone.pth',\n",
        "\t'mtcnn_pnet': 'pretrained_models/mtcnn/pnet.npy',\n",
        "\t'mtcnn_rnet': 'pretrained_models/mtcnn/rnet.npy',\n",
        "\t'mtcnn_onet': 'pretrained_models/mtcnn/onet.npy',\n",
        "\t'shape_predictor': 'pretrained_models/shape_predictor_68_face_landmarks.dat'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDntuv9dZkEq"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeKyxNohVLM0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "rnd = random.randint(0, 1_000_000)\n",
        "print(\"exp_dir:\", f\"exp/{rnd}\")\n",
        "\n",
        "%cd /content/e2style\n",
        "!python scripts/train.py \\\n",
        "    --dataset_type=ffhq_encode \\\n",
        "    --exp_dir=./exp/$rnd \\\n",
        "    --workers=4 \\\n",
        "    --batch_size=4 \\\n",
        "    --test_batch_size=4 \\\n",
        "    --test_workers=4 \\\n",
        "    --val_interval=5000 \\\n",
        "    --save_interval=5000 \\\n",
        "    --start_from_latent_avg \\\n",
        "    --lpips_lambda=0.8 \\\n",
        "    --l2_lambda=1 \\\n",
        "    --id_lambda=0.5 \\\n",
        "    --parse_lambda=1 \\\n",
        "    --training_stage=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWxFOp_aVfX3"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKcS5LYXahlI"
      },
      "outputs": [],
      "source": [
        "# @markdown Move into the `e2style` repository root\n",
        "%cd /content/e2style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY1zP2z2V63c"
      },
      "outputs": [],
      "source": [
        "# @markdown Download a pretrained model\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1CzMDA88GJgVzc5JxKt3-l504a7TuSw5j/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZHRCG-SaYjP"
      },
      "outputs": [],
      "source": [
        "# @markdown Prepare some test data\n",
        "!mkdir test_data\n",
        "!cp $ffhq_dir/00000/00000.png test_data/\n",
        "!cp $ffhq_dir/00000/00001.png test_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsFTObm2V2uX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "rnd = random.randint(0, 1_000_000)\n",
        "print(\"exp_dir:\", f\"exp/{rnd}\")\n",
        "\n",
        "!python scripts/inference.py \\\n",
        "    --exp_dir=./inf/$rnd \\\n",
        "    --checkpoint_path=inversion.pt \\\n",
        "    --data_path=./test_data \\\n",
        "    --test_batch_size=1 \\\n",
        "    --test_workers=4 \\\n",
        "    --stage=1 \\\n",
        "    --save_inverted_codes \\\n",
        "    --couple_outputs \\\n",
        "    --resize_outputs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YNvY8TovA_nc",
        "Z-gI3tb0Bss1",
        "kxIgS8WdB4pQ",
        "e0xuVVJzCTa-",
        "dsj5eYU9GOuj",
        "BJC5oUXL9Eif",
        "ML9GLyFb9JyK",
        "gOzxE6Yd3WSf",
        "BkxszaxaVyeU",
        "cV-b7CB-ZeaY",
        "oN_cxyQqZTai",
        "pruIog4GZZKx",
        "2yHStBF71Ymc",
        "RSr2a6681w6e",
        "U6UZodVx2uEA",
        "IDntuv9dZkEq",
        "VWxFOp_aVfX3"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "89a17144586464b8ad29eb738f315503f40ccf5acd27c4498b60cfa1b9f49e6c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
